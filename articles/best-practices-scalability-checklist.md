<properties
   pageTitle="Liste de contrôle de l’extensibilité | Microsoft Azure"
   description="Liste de contrôle de l’extensibilité et recommandations concernant les problématiques de conception pour la mise à l’échelle automatique d’Azure."
   services=""
   documentationCenter="na"
   authors="dragon119"
   manager="masimms"
   editor=""
   tags=""/>

<tags
   ms.service="best-practice"
   ms.devlang="na"
   ms.topic="article"
   ms.tgt_pltfrm="na"
   ms.workload="na"
   ms.date="12/16/2015"
   ms.author="masashin"/>

# Liste de contrôle de l’extensibilité

![](media/best-practices-scalability-checklist/pnp-logo.png)

## Conception du service
- **Partitionnez la charge de travail**. Concevez les parties du processus de sorte qu’elles soient distinctes et décomposables, et minimisez la taille de chaque partie tout en respectant les règles habituelles de séparation des éléments problématiques et le principe de responsabilité unique. Cela permet de distribuer les composants de manière à optimiser l’utilisation de chaque unité de calcul (telle qu’un serveur de rôles ou de base de données) et facilite la mise à l’échelle de l’application en ajoutant des instances supplémentaires de ressources spécifiques. Pour plus d’informations, consultez la page [Recommandations en matière de partitionnement des unités de calcul](https://msdn.microsoft.com/library/dn568099.aspx).
- **Pensez la conception pour la mise à l’échelle**. La mise à l’échelle permet aux applications de répondre à la variabilité de la charge en augmentant ou en diminuant le nombre d’instances de rôles, de files d’attente et d’autres services qu’elles utilisent. L’application doit toutefois être conçue en ce sens. Par exemple, l’application et les services qu’elle utilise doivent être sans état pour être acheminés vers n’importe quelle instance et de sorte que l’ajout ou la suppression d’instances spécifiques n’ait pas de conséquences néfastes sur les utilisateurs actuels. Il convient également d’implémenter la configuration ou la détection automatique des instances à mesure qu’elles sont ajoutées et supprimées de sorte que le code de l’application puisse effectuer le routage nécessaire. Par exemple, une application web peut utiliser un ensemble de files d’attente selon une approche de type tourniquet (round robin) pour acheminer les demandes vers les services d’arrière-plan exécutés dans les rôles de travail. L’application web doit être en mesure de détecter les modifications du nombre de files d’attente pour pouvoir correctement acheminer les demandes et équilibrer la charge sur l’application.
- **Considérez la mise à l’échelle de manière unitaire**. Prévoyez l’ajout de ressources supplémentaires pour prendre en charge la croissance. Pour chaque ressource, vous devez connaître les limites supérieures de mise à l’échelle et utiliser le partitionnement ou la décomposition pour dépasser ces limites. Déterminez les unités d’échelle pour le système en termes de jeux bien définis de ressources. Ainsi, la mise en œuvre des opérations de montée en charge est plus simple et moins susceptible d’avoir des conséquences néfastes sur l’application à cause de limitations imposées par le manque de ressources dans une partie du système global. Par exemple, l’ajout de x rôles de travail et web peut nécessiter y files d’attente supplémentaires et z comptes de stockage pour gérer la charge de travail supplémentaire générée par les rôles. Une unité d’échelle peut donc comprendre x rôles de travail et web, _y_ files d’attente et _z_ comptes de stockage. Concevez l’application de sorte qu’elle puisse facilement être mise à l’échelle en ajoutant une ou plusieurs unités d’échelle.
- **Évitez l’affinité du client**. Si possible, assurez-vous que l’application ne nécessite pas d’affinité afin que les demandes puissent être acheminées vers n’importe quelle instance et que le nombre d’instances soit sans importance. Cela élimine également la surcharge liée au stockage, à la récupération et à la maintenance des informations d’état pour chaque utilisateur.
- **Tirez parti des fonctionnalités de mise à l’échelle automatique de la plateforme**. Quand la plateforme d’hébergement prend en charge une capacité de mise à l’échelle automatique, telle que la fonction Mise à l’échelle automatique d’Azure, préférez celle-ci aux mécanismes personnalisés ou tiers, à moins que le mécanisme intégré ne soit pas en mesure de satisfaire vos exigences. Utilisez si possible des règles de mise à l’échelle planifiées pour vous assurer que les ressources sont disponibles sans délai de démarrage, mais ajoutez si nécessaire une mise à l’échelle automatique réactive aux règles pour faire face aux évolutions imprévues de la demande. Vous pouvez utiliser les opérations de mise à l’échelle automatique de l’API Service Management pour affiner la mise à l’échelle automatique, et ajouter des compteurs personnalisés aux règles au-delà des options de configuration disponibles dans le portail web. Pour plus d’informations, consultez la page [Recommandations en matière de mise à l’échelle automatique](best-practices-auto-scaling.md).
- **Déchargez les tâches utilisant l’UC et les E/S de manière intensive en tant que tâches en arrière-plan**. Si vous vous attendez à ce qu’une demande à un service mette beaucoup de temps à être exécutée ou absorbe des ressources considérables, déchargez le traitement de cette demande vers une tâche distincte. Utilisez des rôles de travail ou des travaux en arrière-plan (selon la plate-forme d’hébergement) pour exécuter ces tâches. Cette stratégie permet au service de continuer à recevoir des demandes et de rester réactif. Pour plus d’informations, consultez le [Guide relatif aux travaux en arrière-plan](best-practices-background-jobs.md).
- **Distribuez la charge de travail pour les tâches en arrière-plan**. Quand les tâches en arrière-plan sont nombreuses ou nécessitent beaucoup de temps ou de ressources, répartissez la charge de travail entre plusieurs unités de calcul (par exemple, des rôles de travail ou des travaux en arrière-plan). Le [modèle des Consommateurs concurrents](https://msdn.microsoft.com/library/dn568101.aspx) offre une solution possible.
- **Envisagez la migration vers une architecture _sans partage_**. Une architecture sans partage utilise des nœuds indépendants et autonomes qui n’ont aucun point de contention tel que des services partagés ou de stockage. En théorie, un tel système peut être mis à l’échelle presque indéfiniment. Bien qu’une approche sans le moindre partage ne soit généralement pas pratique pour la plupart des applications, elle peut offrir des opportunités de conception pour une meilleure extensibilité. Par exemple, éviter l’utilisation de l’état de session, de l’affinité du client et du partitionnement des données côté serveur est un bon exemple de migration vers une architecture sans partage.

## Gestion des données

- **Utilisez le partitionnement des données**. Répartissez les données entre plusieurs bases de données et serveurs de base de données, ou concevez l’application de sorte qu’elle utilise des services de stockage de données à même de fournir ce partitionnement en toute transparence (par exemple, l’infrastructure élastique de base de données SQL Azure ou le stockage de tables Azure). Cette approche peut contribuer à optimiser les performances et à faciliter la mise à l’échelle. Il existe différentes techniques de partitionnement, telles que le partitionnement horizontal, vertical et fonctionnel, que vous pouvez associer pour tirer le meilleur parti des performances de requête accrues, de l’extensibilité simplifiée, de la gestion plus flexible et de la disponibilité améliorée, tout en faisant correspondre le type de magasin aux données qu’il contiendra. Envisagez également d’utiliser différents types de magasin de données pour les différents types de données, en choisissant ceux qui sont le mieux adaptés aux types de données spécifiques. Cela peut impliquer l’utilisation d’une table de stockage, d’une base de données de documents ou d’un magasin de données colonne-famille au lieu ou en plus d’une base de données relationnelle. Pour plus d’informations, consultez la page [Recommandations en matière de partitionnement](best-practices-data-partitioning.md).
- **Pensez la conception pour la cohérence finale**. La cohérence finale améliore l’extensibilité en réduisant ou en éliminant le temps nécessaire à la synchronisation des données associées partitionnées entre plusieurs magasins. Le problème est que les données ne sont pas toujours cohérentes lorsqu’elles sont lues et que certaines opérations d’écriture peuvent provoquer des conflits. La cohérence finale est idéale pour les situations où les mêmes données sont fréquemment lues mais rarement écrites. Pour plus d’informations, reportez-vous aux [conseils en matière de cohérence des données](#insertlink#).
- **Réduisez les interactions impliquant de nombreux échanges entre les composants et les services**. Évitez de concevoir des interfaces _exigeant de nombreux échanges_ pour les services, où une application doit effectuer plusieurs appels à un service (chaque appel renvoyant une petite quantité de données) plutôt qu’un appel unique pouvant renvoyer l’ensemble des données. Si possible, associez plusieurs opérations connexes dans une seule demande lorsque l’appel effectué est à destination d’un service ou d’un composant présentant une latence importante. Il est ainsi plus facile de surveiller les performances et d’optimiser les opérations complexes. Par exemple, utilisez les procédures stockées dans les bases de données pour encapsuler une logique complexe et réduire le nombre d’allers et retours et le verrouillage de ressources. 
- **Utilisez des files d’attente afin de niveler la charge pour les écritures de données à haute vitesse**. Les pics de demande pour un service peuvent surcharger ce service et provoquer des défaillances toujours plus importantes. Pour éviter ce problème, envisagez d’implémenter le [modèle de nivellement de charge basé sur la file d’attente](https://msdn.microsoft.com/library/dn589783.aspx). Utilisez une file d’attente qui agit comme un tampon entre une tâche et un service qu’elle appelle pour atténuer les surcharges intermittentes qui pourraient autrement entraîner la défaillance du service ou l’expiration de la tâche.
- **Réduisez la charge sur le magasin de données**. Le magasin de données constitue généralement un goulot d’étranglement, une ressource coûteuse et s’avère souvent complexe à monter en charge. Si possible, supprimez la logique (telle que le traitement des documents XML ou des objets JSON) du magasin de données et effectuez le traitement au sein de l’application. Par exemple, au lieu de transmettre le XML à la base de données (autrement que comme une chaîne opaque pour le stockage), sérialisez ou désérialisez le XML au sein de la couche application et transmettez-le dans un format natif pour le magasin de données. Il est généralement plus facile de monter en charge une application que le magasin de données et par conséquent, vous devez essayer d’effectuer autant que possible le traitement nécessitant beaucoup de ressources au sein de l’application.
- **Réduisez le volume de données récupérées**. Récupérez uniquement les données dont vous avez besoin en spécifiant des colonnes et en utilisant des critères pour sélectionner les lignes. Exploitez les paramètres de valeur de table et le niveau d’isolement approprié. Utilisez des mécanismes de type ETags pour éviter de récupérer des données inutiles.
- **Utilisez la mise en cache de manière intensive**. Utilisez la mise en cache autant que possible pour réduire la charge sur les ressources et les services qui génèrent ou fournissent des données. La mise en cache est généralement adaptée aux données relativement statiques ou dont l’obtention nécessite un traitement considérable. La mise en cache doit intervenir à tous les niveaux appropriés dans chaque couche de l’application, y compris l’accès aux données et la génération de l’interface utilisateur. Pour plus d’informations, consultez la page [Conseils de mise en cache](best-practices-caching.md).
- **Gérez la croissance et la rétention des données**. La quantité de données stockées par une application augmente au fil du temps. Cette croissance entraîne l’augmentation des coûts de stockage et de la latence lors de l’accès aux données, ce qui affecte le rendement et les performances de l’application. Il peut être possible d’archiver certaines données anciennes qui ne sont plus utilisées ou de déplacer les données rarement utilisées dans un stockage à long terme plus économique, même si la latence d’accès est supérieure.
- **Optimisez les objets de transfert de données à l’aide d’un format binaire efficace**. Les objets de transfert de données sont transmis de nombreuses fois entre les couches d’une application ; réduire leur taille permet donc de réduire la charge sur les ressources et le réseau. Cependant, il convient d’équilibrer les gains réalisés avec la surcharge liée à la conversion des données au format requis à chaque emplacement où elles sont utilisées et d’adopter le format offrant l’interopérabilité maximale pour faciliter la réutilisation d’un composant.
- **Définissez le contrôle de cache**. Concevez et configurez l’application de sorte qu’elle utilise la mise en cache de sortie ou la mise en cache de fragments dès que possible pour réduire la charge de traitement.
- **Activez la mise en cache côté client**. Les applications web doivent activer les paramètres de cache pour le contenu pouvant être mis en cache. Ces paramètres sont généralement désactivés par défaut. Configurez le serveur de sorte qu’il fournisse les en-têtes de contrôle de cache appropriés pour activer la mise en cache du contenu sur les serveurs proxy et les clients.
- **Utilisez le stockage d’objets blob Azure et le CDN pour réduire la charge sur l’application**. Envisagez de stocker le contenu public statique ou relativement statique tel que les images, les ressources, les scripts et les feuilles de style dans le stockage d’objets blob. Cette approche soulage l’application de la charge causée par la génération dynamique de ce contenu pour chaque demande. Envisagez également d’utiliser le CDN pour mettre en cache ce contenu et le remettre aux clients. L’utilisation du CDN peut améliorer les performances au niveau du client, car le contenu est distribué à partir du centre de données contenant un cache CDN le plus proche géographiquement. Pour plus d’informations, consultez la page [Aide relative au CDN](best-practices-cdn.md).

- **Optimisez et ajustez les requêtes et index SQL**. Certaines instructions ou constructions T-SQL sont susceptibles d’avoir un impact sur les performances, qui peut être réduit en optimisant le code dans une procédure stockée. Par exemple, il convient d’éviter de convertir les types **datetime** en **varchar** avant de les comparer avec une valeur littérale **datetime** : utilisez à la place des fonctions de comparaison de date/heure. L’absence d’index appropriés peut également ralentir l’exécution des requêtes. Si vous utilisez une infrastructure de mappage objet/relationnel (ORM), examinez son fonctionnement et l’impact qu’elle peut avoir sur les performances de la couche d’accès aux données. Pour plus d’informations, consultez la page [Ajustement des requêtes](https://technet.microsoft.com/library/ms176005.aspx).
- **Envisagez de dénormaliser les données**. La normalisation des données permet d’éviter la duplication et les incohérences. Cependant, la maintenance de plusieurs index, la vérification de l’intégrité référentielle, les accès multiples à de petits blocs de données et la jointure des tables pour rassembler les données imposent une surcharge qui peut affecter les performances. Demandez-vous si un volume de stockage et une duplication supplémentaires sont acceptables pour réduire la charge sur le magasin de données. Demandez-vous également si l’application elle-même (qui est généralement plus facile à mettre à l’échelle) offre la fiabilité nécessaire pour prendre le relais dans l’exécution de tâches telles que la gestion de l’intégrité référentielle afin de réduire la charge sur le magasin de données. Pour plus d’informations, consultez la page [Recommandations en matière de partitionnement](https://github.com/mspnp/azure-guidance/blob/master/Data%20partitioning.md).

## Implémentation du service
- **Utilisez des appels asynchrones**. Utilisez autant que possible du code asynchrone lors de l’accès à des ressources ou des services susceptibles d’être limités par les E/S ou la bande passante réseau, ou qui présentent une latence importance, pour éviter le verrouillage du thread appelant. Utilisez le modèle asynchrone basé sur des tâches pour implémenter des opérations asynchrones. Pour plus d’informations, consultez la page [Modèle asynchrone basé sur des tâches (TAP)](https://msdn.microsoft.com/library/hh873175.aspx) du site web de Microsoft.
- **Évitez le verrouillage des ressources et utilisez une approche optimiste à la place**. Ne verrouillez jamais l’accès aux ressources telles que le stockage ou d’autres services présentant une latence importante, car il s’agit d’une des principales causes de faibles performances. Utilisez toujours des approches optimistes de la gestion des opérations simultanées, telles que l’écriture dans le stockage, et faites toujours appel aux fonctionnalités de la couche de stockage pour gérer les conflits. Dans les applications distribuées, il se peut que les données ne soient cohérentes qu’à la fin.
- **Compressez les données hautement compressibles sur les réseaux à latence élevée et faible bande passante**. Dans la plupart des cas dans une application web, le volume le plus important de données générées par l’application et transmises sur le réseau correspond aux réponses HTTP aux demandes des clients. La compression HTTP peut réduire considérablement ce volume, notamment pour le contenu statique. Cela permet de réaliser des économies et de diminuer la charge sur le réseau, même si la compression de contenu dynamique applique une charge légèrement supérieure sur le serveur. Dans d’autres environnements plus généralisés, la compression de données peut réduire le volume de données transmises et minimiser le temps et les coûts de transfert, mais les processus de compression et de décompression entraîneront des frais supplémentaires. Ainsi, la compression ne doit être utilisée que lorsque le gain de performances est avéré. D’autres méthodes de sérialisation (encodage JSON ou binaire, par exemple) peuvent réduire la taille de charge utile tout en ayant moins d’impact sur les performances, tandis que XML est susceptible de l’augmenter.
- **Réduisez le temps d’utilisation des connexions et des ressources**. Maintenez les connexions et les ressources uniquement le temps de les utiliser. Par exemple, ouvrez les connexions le plus tard possible et permettez leur retour dans le pool de connexions le plus tôt possible. Faites l’acquisition des ressources le plus tard possible et défaites-vous en le plus tôt possible.
- **Réduisez le nombre de connexions requises**. Les connexions au service absorbent les ressources. Si possible, limitez le nombre de connexions requises et assurez-vous que les connexions existantes sont réutilisées chaque fois que possible. Par exemple, après l’exécution de l’authentification, utilisez l’emprunt d’identité là où cela est nécessaire pour exécuter du code sous une identité spécifique. Cela peut contribuer à optimiser l’utilisation du pool de connexions en réutilisant les connexions. 

	> [AZURE.NOTE]\: ** les API de certains services réutilisent automatiquement les connexions à condition que les directives spécifiques aux services soient suivies. Il est important de comprendre les conditions qui permettent la réutilisation de la connexion pour chaque service que votre application utilise.
- **Envoyez des demandes par lots pour optimiser l’utilisation du réseau**. Par exemple, envoyez et lisez des messages par lots lorsque vous accédez à une file d’attente et effectuez plusieurs lectures ou écritures par lot lors de l’accès au stockage ou à un cache. Cela peut contribuer à optimiser l’efficacité des services et des magasins de données en réduisant le nombre d’appels sur le réseau.
- **Éliminez si possible la nécessité de stocker l’état de session côté serveur**. La gestion de l’état de session côté serveur requiert généralement l’affinité du client (en d’autres termes, le routage de chaque demande vers la même instance de serveur), ce qui affecte la capacité de mise à l’échelle du système. Dans l’idéal, vous devez concevoir les clients de sorte qu’ils soient sans état par rapport aux serveurs utilisés. Cependant, si l’application doit tenir à jour l’état de session, stockez les données sensibles ou les volumes importants de données par client dans un cache distribué côté serveur auquel toutes les instances de l’application peuvent accéder.
- **Optimisez les schémas de stockage de tables**. Lorsque vous utilisez des magasins de tables, tels que le stockage de tables Azure, qui nécessitent que les noms de tables et de colonnes soient transmis et traités avec chaque requête, envisagez d’utiliser des noms plus courts pour réduire cette surcharge. Cependant, ne sacrifiez pas la lisibilité ou la facilité de gestion en utilisant des noms excessivement compacts.
- **Tirez parti de la bibliothèque parallèle de tâches pour effectuer des opérations asynchrones**. La bibliothèque parallèle de tâches (TPL) facilite l’écriture de code asynchrone qui permet d’effectuer des opérations utilisant des E/S de manière intensive. Utilisez si possible _ConfigureAwait (false)_ pour éliminer la dépendance d’une liaison dans un contexte de synchronisation spécifique et réduire les risques de blocage de thread.
- **Créez des dépendances de ressources lors du déploiement ou au démarrage de l’application**. Évitez les appels répétés à des méthodes qui testent l’existence d’une ressource, puis créent la ressource si elle n’existe pas (les méthodes _CloudTable.CreateIfNotExists_ et _CloudQueue.CreateIfNotExists_ de la bibliothèque cliente de stockage Azure suivent par exemple ce modèle). Ces méthodes peuvent imposer une surcharge considérable si elles sont appelées avant chaque accès à une table de stockage ou à une file d’attente de stockage. Au lieu de cela, créez les ressources requises lorsque l’application est déployée ou qu’elle démarre pour la première fois (un seul appel à _CreateIfNotExists_ pour chaque ressource dans le code de démarrage pour un rôle de travail ou web est acceptable). Veillez toutefois à gérer les exceptions qui peuvent se présenter si votre code tente d’accéder à une ressource inexistante. En pareil cas, vous devez consigner l’exception et potentiellement prévenir un opérateur qu’il manque une ressource. Dans certaines circonstances, il peut s’avérer judicieux de créer la ressource manquante dans le code de gestion des exceptions, mais vous devez adopter cette approche avec précaution, car l’inexistence de la ressource peut être symptomatique d’une erreur de programmation (un nom de ressource mal orthographié, par exemple) ou d’un autre problème au niveau de l’infrastructure.
- **Utilisez des infrastructures légères**. Choisissez soigneusement les API et les infrastructures que vous utilisez afin de réduire l’utilisation des ressources, le délai d’exécution et la charge globale sur l’application. Par exemple, l’utilisation des API Web pour gérer les demandes de service peut réduire l’encombrement des applications et accélérer l’exécution, mais il se peut qu’elle ne convienne pas pour des scénarios avancés où les fonctionnalités supplémentaires de WCF sont requises.
- **Envisagez de réduire le nombre de comptes de service**. Par exemple, utilisez un compte spécifique pour accéder aux ressources ou services qui imposent un nombre limite de connexions ou offrent de meilleures performances avec un nombre réduit de connexions simultanées. Cette approche est courante pour les services tels que les bases de données, mais elle peut affecter la capacité à vérifier avec précision les opérations en raison de l’emprunt d’identité de l’utilisateur d’origine.
- **Procédez au profilage des performances et au test de charge** pendant le développement, dans le cadre des routines de test et avant la publication de la version finale pour vous assurer que l’application fonctionne et est mise à l’échelle en fonction des besoins. Ce test doit être exécuté sur le même type de matériel que la plateforme de production et avec les mêmes types et quantités de données et de charge utilisateur que l’application rencontrera en production. Pour plus d’informations, consultez la page [Test des performances d’un service cloud](https://azure.microsoft.com/documentation/articles/vs-azure-tools-performance-profiling-cloud-services) du site web de Microsoft.

<!---HONumber=AcomDC_0316_2016-->